{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Uczenie ze wzmocnieniem\n",
    "\n",
    "W dotychczasowych wykładach zajmowaliśmy się dwoma skrajnymi typami uczenia: uczenie z nadzorem i bez nadzoru. \n",
    "* W pierwszym typie ciąg uczący zawiera wektory wejściowe i pożądane wartości wyjściowe. Na podstawie takiego zbioru możliwe jest bezpośrednie policzenie błędu popełnionego przez algorytm uczący i wyliczenie konkretnych poprawek.\n",
    "* Drugą skrajnością były algorytmy analizy skupień bazujące na korelacjach między wejściem i wyjściem oraz na strukturze danych wejściowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uczenie z nagrodą i karą\n",
    "W tym wykładzie zajmiemy się sytuacją pośrednią. \n",
    "* Dla każdego wejścia nie znamy co prawda prawidłowej wartości wyjścia, \n",
    "* ale umiemy powiedzieć czy było ono dobre czy złe. \n",
    "\n",
    "_Przypomina to trochę uczenie psa aportowania. Nie da się psu wytłumaczyć jak ma to zrobić, jedynie w wypadku powodzenia pies dostaje pochwałę/nagrodę a w przypadku niepowodzenia nic._\n",
    "\n",
    "Podobnie w uczeniu ze wzmocnieniem będziemy jedynie wiedzieli kiedy algorytm nagrodzić (będziemy do tego używać funkcji nagrody) a kiedy karać."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Przypisanie kary do kroku\n",
    "Specyficznym problemem jest tu przypisanie kary za błąd. \n",
    "\n",
    "* Zwykle w zadaniach których rozwiązania chcielibyśmy nauczyć maszynę występuje wiele etapów i nie jest do końca jasne, który z nich przyczynił się do ostatecznego błędu. Można tu jako przykład rozważyć grę w szachy. \n",
    "  * Jeśli po 60 posunięciach partia się kończy naszą przegraną to nie jest jasne, który z naszych ruchów był nieprawidłowy.\n",
    "\n",
    "Algorytmy tego typu znajdują bardzo wiele praktycznych zastosowań np: algorytmy do gier strategicznych, sterowanie robotami, optymalizacja przesyłania danych w sieciach komórkowych, efektywne indeksowanie stron internetowych itp.\n",
    "\n",
    "Zaczniemy od omówienia procesu decyzyjnego Markowa (ang. Markov decision process MDP), który dostarcza formalizmu do opisu uczenia ze wzmocnieniem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Proces decyzyjny Markowa\n",
    "\n",
    "Aby mówić o procesie decyzyjnym Markowa musimy zdefiniować następujący zestaw (krotkę): $(S,A,\\lbrace P_{sa}\\rbrace ,\\gamma ,R)$ gdzie:\n",
    "* $S$ jest zbiorem możliwych stanów, w jakich może znajdować się rozważany układ.\n",
    "* $A$ to zbiór możliwych akcji\n",
    "* $P_{s,a}$ to rozkład prawdopodobieństwa przejścia z bieżącego stanu do każdego z możliwych stanów, tak, że $P_{s,a_{i}}(s^{\\prime })$ określa prawdopodobieństwo z jakim układ znajdujący się w stanie $s$ po wyborze akcji $a_{i}$ znajdzie się w stanie $s^{\\prime }$.\n",
    "* $\\gamma $ to współczynnik o wartości $[0,1)$ służący do rozdzielania kary/nagrody pomiędzy kroki\n",
    "* $R: S\\times A \\mapsto \\mathcal {R}$ to funkcja nagrody."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Strategia\n",
    "* Aby \"uruchomić\" MDP potrzebna jest <i>strategia</i>, czyli mechanizm wybierania akcji.\n",
    "* Formalnie strategia $\\pi : S \\mapsto A$ to funkcja, która mapuje stany na akcje. \n",
    "  * Czyli jeśli znajdujemy się w stanie $s$ to akcje wybieramy tak: $ a= \\pi (s)$.\n",
    "* Działanie MDP można opisać następująco: \n",
    "  * Początkowo układ znajduje się w stanie $s_{0}$. \n",
    "  * Zgodnie ze strategią $\\pi (s_{0})$ wybierana jest akcja $a_{0}$. \n",
    "  * W sposób losowy, zgodnie z rozkładem $P_{s_{0},a_{0}}$ akcja $a_{0}$ przenosi układ do stanu $s_{1}$.\n",
    "  * Następnie strategia $\\pi (s_{1})$ decyduje jaką akcję $a_{1}$ wykonamy i z prawdopodobieństwem $P_{s_{1},a_{1}}$ przejdziemy do stanu $s_{2}$, \n",
    "  * itd. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Na obrazku wygląda to tak:\n",
    "\n",
    "$s_0 \\, ^{\\underrightarrow{a_0} }\\, s_1 \\, ^{\\underrightarrow{a_1} }\\,  s_2 \\, ^{\\underrightarrow{a_2} }\\,  s_3\\, ^{\\underrightarrow{a_3} }\\,   \\dots $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Przykład\n",
    "Aby te pojęcia nabrały znaczenia rozważmy konkretny przykład. Mamy robota, \"żyjącego\" w świecie złożonym z 12 pól takim jak na poniższym rysunku.\n",
    "![](https://brain.fuw.edu.pl/edu/images/8/87/Swiat_robota.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Zajmowanie przez robota pola o konkretnych współrzędnych to jego stan, np. jeśli robot znajduje się w lewym górnym rogu to jest w stanie $s = (x=1,y=3)$. \n",
    "* Jego zbiór akcji to przemieszczenie się o jedną pozycję w jednym z czterech kierunków: $A = \\lbrace N,E,S,W\\rbrace $. \n",
    "* Robot nie może wejść na czarne pole, ani przeniknąć przez ściany. Próba podjęcia takiej niemożliwej akcji skutkuje tym, że robot pozostaje w dotychczasowym stanie.\n",
    "* Chcielibyśmy, aby robot uruchomiony w takim świecie dotarł do stanu $(4,3)$ i jak tylko to możliwe unikał stanu $(4,2)$. \n",
    "* Przebywanie w każdym ze stanów związane jest z funkcją nagrody, zaznaczoną kolorami. \n",
    "  * Dla ustalenia uwagi weźmy funkcję nagrody taką, że:\n",
    "    * stan $(4,3)$ ma nagrodę $R((4,3)) = +1$, \n",
    "    * $R((4,2)) = -1$, \n",
    "    * pozostałe stany mają wartość $R = -0.02$. \n",
    "  * Widać, że możemy nadawać funkcji nagrody wartości dodatnie (za przebywanie systemu w stanach, które uznajemy za korzystne) lub ujemne (w stanach mniej korzystnych).\n",
    "* Dodatkowo załóżmy, że dynamika naszego robota jest stochastyczna. Innymi słowy ruchy naszego robota nie są idealnie precyzyjne, \n",
    "  * np. jeśli dostanie polecenie $N$ to przemieści się do góry z prawdopodobieństwem powiedzmy 0.8, na lewo z prawdopodobieństwem 0.1 i na prawo z prawdopodobieństwem 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Przy takiej dynamice rozkład prawdopodobieństwa przejścia między stanami może wyglądać np. tak:\n",
    "\n",
    "$P_{(3,1),N}(3,2) = 0.8$ (Notacja ta oznacza prawdopodobieństwo przejścia ze stanu (3,1) do (3,2) po wybraniu akcji $N$)\n",
    "\n",
    "$P_{(3,1),N}(2,1) = 0.1$\n",
    "\n",
    "$P_{(3,1),N}(4,1) = 0.1$\n",
    "\n",
    "$P_{(3,1),N}(3,3) = 0$\n",
    "\n",
    "$\\vdots $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Rozważmy teraz przykładowe działanie naszego robota, przyjmując strategię taką jak na poniższym rysunku.\n",
    "\n",
    "\n",
    "\n",
    "<table class=\"wikitable\";style = font-size:200%>\n",
    "\t<tr>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>+1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>N</td>\n",
    "\t\t<td>•</td>\n",
    "\t\t<td>N</td>\n",
    "\t\t<td>-1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>N</td>\n",
    "\t\t<td>W</td>\n",
    "\t\t<td>W</td>\n",
    "\t\t<td>W</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* Umieszczamy go w chwili $t_{0}$ w jakimś stanie $s_{0}$ (tzn. na którymś konkretnym polu)\n",
    "\n",
    "* Wybieramy akcję $a_{0}$\n",
    "\n",
    "* W zależności od $s_{0}$ i $a_{0}$ robot znajdzie się w stanie $s_{1}$ (Stan $s_{1}$ jest losowany z rozkładu prawdopodobieństwa $P_{s_{0},a_{0}}$ ; możemy to zapisać $s_{1} \\sim P_{s_{0},a_{0}}$)\n",
    "\n",
    "* Wybieramy akcję $a_{1}$\n",
    "\n",
    "* Losujemy stan $s_{2} \\sim P_{s_{1},a_{1}}$\n",
    "\n",
    "* $\\dots $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Zysk\n",
    "Z przebyciem konkretnej sekwencji stanów związany jest całkowity zysk, który może być obliczony z funkcji nagrody w następujący sposób:\n",
    "\n",
    "$$Z = R(s_{0}) + \\gamma R(s_{1})+ \\gamma ^{2} R(s_{2}) + \\dots $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Widzimy, że współczynnik $\\gamma $ odpowiadający krokowi $k$ występuje w potędze $k$. \n",
    "\n",
    "* Ponieważ jest on dodatni i mniejszy niż 1 oznacza to, że na wartość oczekiwaną zysku mają największy wpływ wartości funkcji nagrody w najwcześniejszych korkach. \n",
    "\n",
    "* Możemy o tym myśleć w analogii do pieniędzy: Dobrze ulokowane pieniądze przynoszą tym większy zysk im wcześniej je ulokowaliśmy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cel uczenia ze wzmcnieniem\n",
    "Naszym celem w uczeniu ze wzmocnieniem jest takie wybieranie akcji (znalezienie takiej strategii) aby zmaksymalizować wartość oczekiwaną całkowitego zysku:\n",
    "\n",
    "$$E [Z] =E [R(s_{0}) + \\gamma R(s_{1})+ \\gamma ^{2} R(s_{2}) + \\dots ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jak wyznaczyć optymalną strategię?\n",
    "### Funkcja wartościująca\n",
    "Pora wprowadzić pojęcie funkcji wartościującej, która posłuży do oceniania strategii: \n",
    "\n",
    "**Funkcja wartościująca dla danej strategii $\\pi $ jest to wartość oczekiwana całkowitego zysku dla przypadku gdy startujemy ze stanu $s_{0}$ i podejmujemy akcje zgodnie ze strategią $\\pi $.**\n",
    "\n",
    "Można to zapisać następująco:\n",
    "\n",
    "\n",
    "$$\n",
    "V^{\\pi }(s) = E[R(s_{0}) + \\gamma R(s_{1})+ \\gamma ^{2} R(s_{2}) + \\dots | s_{0}=s, \\pi ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dla przykładu policzmy tą funkcje dla konkretnej strategii:\n",
    "Strategia:\n",
    "\n",
    "\n",
    "<table class=\"wikitable\";style = font-size:100%>\n",
    "\t<tr>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>+1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>S</td>\n",
    "\t\t<td>•</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>-1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>N</td>\n",
    "\t\t<td>N</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Funkcja wartościująca dla tej strategii:\n",
    "\n",
    "\n",
    "<table class=\"wikitable\";style = font-size:100%>\n",
    "\t<tr>\n",
    "\t\t<td>0.52</td>\n",
    "\t\t<td>0.73</td>\n",
    "\t\t<td>0.77</td>\n",
    "\t\t<td>+1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>-0.90</td>\n",
    "\t\t<td>•</td>\n",
    "\t\t<td>-0.82</td>\n",
    "\t\t<td>-1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>-0.88</td>\n",
    "\t\t<td>-0.87</td>\n",
    "\t\t<td>-0.85</td>\n",
    "\t\t<td>-1</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Równanie poprzednie można przepisać w postaci:\n",
    "\n",
    "$\\qquad$ $$V^{\\pi }(s) = E[R(s_{0}) + \\gamma \\left( R(s_{1})+ \\gamma ^{1} R(s_{2}) + \\gamma ^{2} R(s_{3})+ \\dots \\right) | s_{0}=s, \\pi ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ponieważ $R(s_{0})$ jest stałe to mamy:\n",
    "\n",
    "$\\qquad$ $$V^{\\pi }(s) = R(s_{0}) + \\gamma E[\\left( R(s_{1})+ \\gamma ^{1} R(s_{2}) + \\gamma ^{2} R(s_{3})+ \\dots \\right) | s_{0}=s, \\pi ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zauważmy, że wyrażenie: \n",
    "\n",
    "$\\qquad$ $$ E[R(s_{1})+ \\gamma ^{1} R(s_{2}) + \\gamma ^{2} R(s_{3})+ \\dots] $$\n",
    "\n",
    "to funkcja wartościująca strategi dla stanu $s_{1}$: $V^{\\pi }(s_{1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Przechodząc do notacji: \n",
    "\n",
    "$s_{0} \\rightarrow s$ i $s_{1} \\rightarrow s^{\\prime }$ \n",
    "\n",
    "możemy funkcję wartościującą strategii wyrazić w sposób rekurencyjny:\n",
    "\n",
    "$\\qquad$$V^{\\pi }(s) = E[R(s) + \\gamma V^{\\pi }(s^{\\prime }) | s_{0}=s, \\pi ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W tym wyrażeniu $s$ jest już ustalone więc \n",
    "* $E[R(s)] = R(s)$ \n",
    "natomiast \n",
    "* $s^{\\prime }$ jest zmienną losową pochodzącą z rozkładu $P_{s, \\pi (s)}$ więc \n",
    "$$E[V^{\\pi }(s^{\\prime }) ] = \\sum _{s^{\\prime }} P_{s, \\pi (s)}(s^{\\prime }) V^{\\pi }(s^{\\prime }) $$.\n",
    "\n",
    "Zbierając razem te obserwacje i podstawiając je do równania otrzymujemy tzw. **równanie Bellmana**:\n",
    "\n",
    " $$V^{\\pi }(s) = R(s) +\\gamma \\sum _{s^{\\prime } \\in S} P_{s,\\pi (s)}(s^{\\prime }) V^{\\pi }(s^{\\prime })$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W szczególności dla MDP o skończonej liczbie stanów $N$ można napisać równanie Bellmana dla każdego stanu $s$. Otrzymamy wtedy układ $N$ równań liniowych z $N$ niewiadomymi ($V^{\\pi }(s)$) , który można łatwo rozwiązać.\n",
    "Popatrzmy na przykład z robotem.\n",
    "Weźmy stan $(3,1)$ i strategię $\\pi ((3,1)) = N$. Równanie Bellmana dla tego stanu to:\n",
    "\n",
    "$$V^{\\pi }\\left( (3,1) \\right) = R((3,1)) + \\gamma [ 0.8 V^{\\pi }((3,2)) +0.1 V^{\\pi }((4,1)) +0.1 V^{\\pi }((2,1)) ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dla każdego z 11 stanów można napisać takie równanie. Otrzymamy 11 równań zawierających 11 niewiadomych $V^{\\pi }(i,j)$.\n",
    "\n",
    "Zdefiniujmy optymalną funkcję wartościującą strategi jako:\n",
    "\n",
    "$$\n",
    "V^{*}(s) = \\max _{\\pi } V^{\\pi }(s)\n",
    "$$\n",
    "\n",
    "Innymi słowy jest to największa możliwa suma nagród (ważonych przez $\\gamma $) przy pomocy najlepszej strategii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Można napisać równanie Bellmana dla $V^{*}(s)$:\n",
    "\n",
    "$\\qquad$$V^{*}(s) = R(s) +\\max _{a}\\gamma \\sum _{s^{\\prime } \\in S} P_{s,a}(s^{\\prime }) V^{*}(s^{\\prime })$\n",
    "\n",
    "* Pierwszy człon to natychmiastowa nagroda.\n",
    "* W drugim członie suma wyraża wartość oczekiwaną zysku jeśli wybralibyśmy akcję $a$. Wartość ta jest maksymalizowana po $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To prowadzi do definicji strategii $\\pi ^{*}(s)$, czyli takiej strategii, która maksymalizuje zysk:\n",
    "\n",
    "$$\n",
    "\\pi ^{*}(s) = \\arg \\max _{a} \\sum _{s^{\\prime }} P_{sa}(s^{\\prime }) V^{*}(s^{\\prime })\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W definicji tej nie musimy już uwzględniać $\\gamma $ bo nie ma ona znaczenia dla operatora $\\arg \\max $. Taka definicja powoduje, że akcje podejmowane zgodnie ze strategią $\\pi ^{*}$ prowadzą do maksymalizacji drugiego członu w równaniu Bellmana na $V^{*}$. W tym sensie strategia $\\pi ^{*}$ jest strategią optymalną. Zauważmy, że $\\pi ^{*}(s)$ jest strategią optymalną bez względu na to, z którego stanu uruchamiany jest proces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorytmy znajdowania optymalnej strategii:\n",
    "\n",
    "Powyższe definicje nie dają niestety przepisu jak obliczyć $\\pi ^{*}$. \n",
    "\n",
    "Właściwe algorytmy omówimy poniżej. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorytm iteracji funkcji wartościującej\n",
    "Zauważmy, że gdybyśmy znali $V^{*}$ to można by je podstawić do ostatniego równania aby wyznaczyć $\\pi ^{*}$.\n",
    "\n",
    "Zatem pierwszy algorytm, który prowadzi do wyznaczenia optymalnej strategi to:\n",
    "**iteracja funkcji wartościującej**:\n",
    "\n",
    "1. inicjuj dla każdego $s$: $V(s) = 0$ (wypełniamy tablicę V zerami)\n",
    "\n",
    "2. Powtarzaj aż zbiegniesz:\n",
    "{\n",
    "\n",
    "dla każdego stanu $s$\n",
    "$$V(s) = R(s) + \\max _{a} \\gamma \\sum _{s^{\\prime }} P_{sa}(s^{\\prime }) V(s^{\\prime }) $$\n",
    "\n",
    "}\n",
    "\n",
    "Tak iterowane $V(s) \\rightarrow V^{*}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* W algorytmie tym można zastosować uaktualnianie synchroniczne - wtedy najpierw obliczamy wszystkie prawe strony, wyniki obliczeń przechowując w zmiennej tymczasowej i dopiero po przebiegnięciu pętlą przez wszystkie stany następuje przypisanie uaktualniające.\n",
    "* Druga możliwość to uaktualnianie asynchroniczne, tzn. w każdym kroku wybieramy jeden stan i uaktualniamy $V(s)$ dla tego stanu. W kolejnym kroku do obliczeń używamy już tego uaktualnionego $V$.\n",
    "* Algorytm asynchroniczny jest zwykle nieco szybciej zbieżny niż synchroniczny. Po wyznaczeniu $V^{*}$ można go podstawić do równania i wyznaczyć optymalną strategię."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Stosując ten algorytm do naszego przykładu można otrzymać np. taka funkcję wartościującą $V^{*}$:\n",
    "\n",
    "\n",
    "<table class=\"wikitable\";style = font-size:100%>\n",
    "\t<tr>\n",
    "\t\t<td>0.86</td>\n",
    "\t\t<td>0.90</td>\n",
    "\t\t<td>0.93</td>\n",
    "\t\t<td>+1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>0.82</td>\n",
    "\t\t<td>•</td>\n",
    "\t\t<td>0.69</td>\n",
    "\t\t<td>-1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>0.78</td>\n",
    "\t\t<td>0.73</td>\n",
    "\t\t<td>0.71</td>\n",
    "\t\t<td>0.49</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Podstawiając do równania na $\\pi ^{*}$ dostajemy, przykładowo dla stanu s= (3,1):\n",
    "\n",
    "Dla akcji W:\n",
    "\n",
    "$\\qquad$$\\sum _{s^{\\prime }} P_{sa}(s^{\\prime })V^{*}(s^{\\prime }) = 0.8\\cdot 0.73 + 0.1\\cdot 0.69 + 0.1\\cdot 0.71 = 0.724 $\n",
    "\n",
    "Dla akcji N:\n",
    "\n",
    "$\\qquad$$\\sum _{s^{\\prime }} P_{sa}(s^{\\prime })V^{*}(s^{\\prime }) = 0.8\\cdot 0.69 + 0.1\\cdot 0.73 + 0.1\\cdot 0.49 = 0.674 $\n",
    "\n",
    "Dla akcji E:\n",
    "\n",
    "$\\qquad$$\\sum _{s^{\\prime }} P_{sa}(s^{\\prime })V^{*}(s^{\\prime }) = 0.8\\cdot 0.49 + 0.1\\cdot 0.69 + 0.1\\cdot 0.71 = 0.532 $\n",
    "\n",
    "Dla akcji S:\n",
    "\n",
    "$\\qquad$$\\sum _{s^{\\prime }} P_{sa}(s^{\\prime })V^{*}(s^{\\prime }) = 0.8\\cdot 0.71 + 0.1\\cdot 0.73 + 0.1\\cdot 0.49 = 0.690 $\n",
    "\n",
    "Zatem dla tego stanu wybrana byłaby akcja W. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Analogiczne obliczenia trzeba przeprowadzić dla pozostałych stanów. Wyniki końcowe prezentuje poniższa tabela.\n",
    "\n",
    "\n",
    "<table class=\"wikitable\";style = font-size:100%>\n",
    "\t<tr>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>E</td>\n",
    "\t\t<td>+1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>N</td>\n",
    "\t\t<td>•</td>\n",
    "\t\t<td>N</td>\n",
    "\t\t<td>-1</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>N</td>\n",
    "\t\t<td>W</td>\n",
    "\t\t<td>W</td>\n",
    "\t\t<td>W</td>\n",
    "\t</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorytm iteracji strategii\n",
    "\n",
    "Poniżej zaprezentowany jest algorytm iteracji strategii.\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\tinicjujemy $\\pi $ w sposób losowy.\n",
    "\t<li>\n",
    "\tPowtarzaj, aż zbiegniesz:\n",
    "{\n",
    "\n",
    "rozwiąż równania Bellmana dla aktualnej strategii (dla procesów o bardzo dużej ilości stanów może to być nieefektywne):\n",
    "\n",
    "$\\qquad$$V = V^{\\pi }$\n",
    "\n",
    "Następnie oblicz lepsze $\\pi $:\n",
    "\n",
    "$\\qquad$$\\pi (s) = \\arg \\max _{a} \\sum _{s^{\\prime }}P_{sa}(s^{\\prime }) V(s^{\\prime })$\n",
    "\n",
    "}\n",
    "\n",
    "</ol>\n",
    "\n",
    "Okazuje się, że powyższa iteracja daje $V \\rightarrow V^{*}$ zaś $\\pi \\rightarrow \\pi ^{*}$.\n",
    "\n",
    "Dla małych i średnich problemów MDP iteracja strategii jest szybko zbieżna, ale dla dużych problemów staje się kosztowna obliczeniowo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorytm aktualizacji funkcji Q\n",
    "Ostatni algorytm ma znamiona eksploracji środowiska. Potrzebna jest do tego funkcja jakości $Q$ (quality). Funkcja $Q$ zwraca jakość dla kombinacji stan-akcja:\n",
    "\n",
    "$$Q: S \\times A \\to \\mathbb{R}$$ \n",
    "\n",
    "Jakość ta mierzona jest przez oczekiwaną wartość całkowitej nagrody za wykonanie danej akcji $a$ w stanie $s$.\n",
    "\n",
    "Czyli dla optymalnej funkcji $Q^*$ i $V^*$ zachodzi związek:\n",
    "$$V^*(s) = \\max_a Q^*(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Najlepszą strategią jest zatem wybieranie akcji  $\\arg \\max_a Q^*(s,a) $:\n",
    "$$\\forall_s \\quad \\pi^*(s) = \\arg \\max_a Q^*(s,a) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Funkcję $Q$ można znaleźć poprzez aktualizacje ważoną średnią dotychczasowej wartości i nowej informacji:\n",
    "<b>iteracja funkcji Q</b>:\n",
    "\n",
    "1. inicjuj dla każdego $s$: $Q(s,a) = 0$ (wypełniamy tablicę Q zerami)\n",
    "\n",
    "2. Powtarzaj aż zbiegniesz:\n",
    "{\n",
    "* Robot ustawiany jest w losowej pozycji. \n",
    "* Przejdź epizod:\n",
    "  * w każdej kolejnej chwili czasu  $t$ robot wybiera akcję  $a_t$, \n",
    "  * dostaje nagrodę $R_t$, \n",
    "  * przechodzi do nowego stanu $s_{t+1}$ ( to może zależeć od poprzedniego stanu  $s_t$ i wybranej akcji), \n",
    "  * $Q$ jest uaktualniane:\n",
    "$$Q(s_{t},a_{t}) := (1-\\alpha) \\cdot \\underbrace{Q(s_{t},a_{t})}_{\\textrm{ stara wartość}} + \\alpha \\cdot  \\overbrace{\\bigg( R_{t} +{\\gamma} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\textrm{ estymata optymalnej przyszłej wartości}} \\bigg) }^{\\textrm{nowa informacja o wartości pary } (s_t,a_t)}$$ \n",
    "  * gdzie ''$R_{t}$'' nagroda w bieżącym stanie $s_t$, i $\\alpha$ jest prędkością uczenia ($0 < \\alpha \\le 1$).\n",
    "  * Epizod nauki kończy się gdy $s_{t+1}$ jest stanem końcowym.\n",
    "\n",
    "Przykład: https://drive.google.com/file/d/18j357je8pSXi3SOYZgK1qp1KWrMsEc2Z/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Estymowanie modelu dla MDP\n",
    "\n",
    "Na początku powiedzieliśmy, że MDP jest określony przez podanie następującej krotki: $(S,A,\\lbrace P_{sa}\\rbrace ,\\gamma ,R)$.\n",
    "\n",
    "* Stany i możliwe akcje zwykle wynikają w sposób naturalny z rozważanego problemu. \n",
    "* $\\gamma $ i $R$ wybieramy sami. \n",
    "* Najtrudniejsze do wymyślenia <i>ad hoc</i> są prawdopodobieństwa przejść. \n",
    "  * Można jednak wyestymować je z danych (przez obserwację wielu realizacji procesu). Używamy wtedy estymatora największej wiarygodności, tzn:\n",
    "\n",
    "$$P_{sa}(s^{\\prime }) = \\frac{\\#\\text{ kiedy wybrano akcje }a\\text{ w stanie }s\\text{ i otrzymano stan }s^{\\prime }}{\\#\\text{ kiedy wybrano akcję }a\\text{ w stanie }s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Zatem dla MDP o nieznanych prawdopodobieństwach przejść można zastosować następujący algorytm:\n",
    "\n",
    "* inicjalizuj $\\pi $ losowo\n",
    "* powtarzaj aż zbiegniesz:\n",
    "{\n",
    "  *\tWykonaj strategię $\\pi $ w MDP dla pewnej liczby prób.\n",
    "  *\tNa podstawie zaobserwowanych sekwencji stanów estymuj $P_{sa}$\n",
    "  *\tzastosuj algorytm iteracji funkcji wartościującej aby oszacować $V$\n",
    "  *\tUaktualnij $\\pi $\n",
    "}\n",
    "\n",
    "W powyższym zastosowaniu algorytm iteracji funkcji wartościującej można zmodyfikować tak, aby nie zaczynał on za każdym razem inicjalizacji od $V(s) = 0$, ale od $V$ które uzyskano w poprzednim kroku zewnętrznej pętli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prośba o wypełnienie ankiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif",
   "transition": "fade",
   "width": 1600
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
