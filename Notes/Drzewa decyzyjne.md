Drzewa decyzyjne to algorytmy, ktÃ³re wykorzystuje siÄ™ w klasyfikacji i regresji. SÄ… popularne dziÄ™ki swojej prostocie, interpretowalnoÅ›ci i wszechstronnoÅ›ci.

Kluczowe cechy:
- Struktura drzewa: skÅ‚ada siÄ™ z wÄ™zÅ‚Ã³w decyzyjnych (testowanie cech) i liÅ›ci (wynik predykcji).
- WszechstronnoÅ›Ä‡: obsÅ‚uguje dane numeryczne i kategoryczne.
- InterpretowalnoÅ›Ä‡: wynik moÅ¼na Å‚atwo wizualizowaÄ‡ i zrozumieÄ‡.
- Typ algorytmu: dziaÅ‚a w trybie wsadowym (batch) i wymaga minimalnego dostrajania parametrÃ³w.
- PodziaÅ‚ przestrzeni cech: tworzy hiperpowierzchnie rÃ³wnolegÅ‚e do osi cech.
![[Pasted image 20250119120430.png]]
## OgÃ³lny proces tworzenia drzew

**PodziaÅ‚ przestrzeni cech**:
- Dzielimy przestrzeÅ„ predyktorÃ³w na **$J$ rozÅ‚Ä…cznych obszarÃ³w** $R_1, R_2, \dots, R_J$â€‹.
- Dla kaÅ¼dej obserwacji w obszarze $R_j$ przewidujemy wartoÅ›Ä‡ bÄ™dÄ…cÄ… Å›redniÄ… odpowiedzi dla obserwacji treningowych w $R_j$â€‹.

**Minimalizacja bÅ‚Ä™du**:
- W klasyfikacji: minimalizujemy **nieczystoÅ›Ä‡** (np. entropia, indeks Gini).
- W regresji: minimalizujemy **sumÄ™ kwadratÃ³w reszt (RSS)**: $$\sum_{j=1}^J\sum_{i \in R_j}(y_i - \hat{y}_{R_j})^2,$$ gdzieÂ  $\hat{y}{R_j}$Â  to Å›rednia wartoÅ›Ä‡ odpowiedzi w obszarzeÂ  $R_j$.

**Niestety, rozwaÅ¼enie kaÅ¼dego moÅ¼liwego podziaÅ‚u przestrzeni cech na R_j jest niewykonalne obliczeniowo.**

### Algorytm Rekurencyjnego PodziaÅ‚u Binarnego: top-down, greedy approach

â€¢ **Top-down**: Drzewo rozwija siÄ™ od korzenia, dzielÄ…c przestrzeÅ„ cech w kolejnych krokach.
â€¢ **Greedy**: Na kaÅ¼dym etapie wybierany jest najlepszy podziaÅ‚ z punktu widzenia bieÅ¼Ä…cego kroku.

**PodziaÅ‚:**
- Wybieramy cechÄ™Â  $X_j$Â  i wartoÅ›Ä‡ progowÄ…Â  $s$ , ktÃ³re minimalizujÄ… RSS po podziale: $$ R_1(j, s) = {X | X_j < s}, \quad R_2(j, s) = {X | X_j \geq s}$$
- Minimalizujemy wyraÅ¼enie: $$\sum_{i:x_i \in R_1(j, s)}(y_i - \hat{y}_{R_1})^2 + \sum_{i:x_i \in R_2(j, s)}(y_i - \hat{y}_{R_2})^2$$
**Kryterium stopu:**
- Proces zatrzymuje siÄ™, gdy:
	- ObszarÂ  $R_j$Â  ma mniej niÅ¼ minimalnÄ… liczbÄ™ obserwacji (np. 5).
	- Dalsze podziaÅ‚y nie zmniejszajÄ… znaczÄ…co RSS.


## Przycinanie Drzewa

Proces rekurencyjnego binarnego dzielenia moÅ¼e prowadziÄ‡ do stworzenia bardzo duÅ¼ego drzewa, ktÃ³re Å›wietnie dopasowuje siÄ™ do danych treningowych, ale jest podatne na przeuczenie (overfitting). Przycinanie drzewa pozwala na zmniejszenie zÅ‚oÅ¼onoÅ›ci modelu, co moÅ¼e poprawiÄ‡ jego ogÃ³lnÄ… wydajnoÅ›Ä‡ na zbiorze testowym.

### Post-prunning

**Post-prunning** polega na budowie peÅ‚nego drzewa decyzyjnego $T_0$, a nastÄ™pnie przyciÄ™cie go w celu uzyskania poddrzewa. W tym celu iteracyjnie przycinamy najsÅ‚absze gaÅ‚Ä™zie (te, ktÃ³re sÄ… najmniej istotne). 

Dla ustalonego parametru $\alpha$ (parametr regularyzacji) wybieramy poddrzewo $T$ drzewa $T_0$ otrzymanego metodÄ… rekurencyjnego binarnego dzielenia tak, aby zminimalizowaÄ‡ koszt:$$ C_\alpha(T) = \sum_{m=1}^{|T|}\sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha|T|$$ gdzie $|T|$ â€“ liczba liÅ›ci w drzewie $T$

**Algorytm:**
1. Zbuduj peÅ‚ne drzewo $T_0$.
2. Iteracyjnie przycinaj najsÅ‚absze gaÅ‚Ä™zie, ktÃ³re powodujÄ… najmniejszy wzrost czÅ‚onu $\sum_{m=1}^{|T|}\sum_{i: x_i \in R_m} (y_i - \hat{y}_{R_m})^2$.
3. Powtarzaj, aÅ¼ uzyskasz korzeÅ„ drzewa. To generuje sekwencjÄ™ poddrzew.
4. Wybierz optymalne poddrzewo $T_\alpha$, uÅ¼ywajÄ…c K-krotnej walidacji krzyÅ¼owej.

<aside> ğŸŒŸ ZaletÄ… metody post-pruning jest dokÅ‚adna kontrola zÅ‚oÅ¼onoÅ›ci drzewa i jej teoretyczne podstawy. Jednak proces iteracyjny moÅ¼e byÄ‡ czasochÅ‚onny. </aside>
### Rule post-prunning

Zamiast usuwaÄ‡ gaÅ‚Ä™zie drzewa, zamienia siÄ™ je w warunki logiczne odpowiadajÄ…ce liÅ›ciom. Te reguÅ‚y mogÄ… byÄ‡ nastÄ™pnie sortowane i redukowane, co poprawia interpretowalnoÅ›Ä‡ modelu. ReguÅ‚y o niskiej jakoÅ›ci sÄ… usuwane, a podobne reguÅ‚y Å‚Ä…czone w celu uproszczenia.

**Algorytm:**
1. PrzeksztaÅ‚Ä‡ kaÅ¼de poddrzewo w reguÅ‚y decyzyjne, gdzie kaÅ¼da Å›cieÅ¼ka od korzenia do liÅ›cia reprezentuje jednÄ… reguÅ‚Ä™.
2. OceÅ„ jakoÅ›Ä‡ reguÅ‚, np. na podstawie wskaÅºnikÃ³w takich jak:
    - WspÃ³Å‚czynnik pokrycia (_coverage_): liczba obserwacji, ktÃ³re speÅ‚niajÄ… reguÅ‚Ä™.
    - WspÃ³Å‚czynnik dokÅ‚adnoÅ›ci (_accuracy_): stosunek poprawnych predykcji do wszystkich przypadkÃ³w speÅ‚niajÄ…cych reguÅ‚Ä™.
3. UsuÅ„ reguÅ‚y o niskiej jakoÅ›ci (np. o zbyt niskim pokryciu lub dokÅ‚adnoÅ›ci).
4. JeÅ›li kilka reguÅ‚ prowadzi do tej samej decyzji, poÅ‚Ä…cz je w jednÄ… reguÅ‚Ä™ (redukcja redundancji).

<aside> ğŸŒŸ ZaletÄ… metody rule post-pruning jest Å‚atwoÅ›Ä‡ interpretacji i moÅ¼liwoÅ›Ä‡ efektywnego dopasowania do rzeczywistych danych. Jednak moÅ¼e byÄ‡ mniej skuteczna w optymalizacji zÅ‚oÅ¼onoÅ›ci drzewa w porÃ³wnaniu do post-pruning. </aside>


## Algorytm konstrukcji drzewa regresyjnego

1. UÅ¼yj rekurencyjnego podziaÅ‚u binarnego do budowy duÅ¼ego drzewa na danych treningowych, zatrzymujÄ…c siÄ™ tylko wtedy, gdy kaÅ¼dy wÄ™zeÅ‚ terminalny zawiera mniej niÅ¼ pewnÄ… minimalnÄ… liczbÄ™ obserwacji.
    
2. Zastosuj metodÄ™ przycinania najsÅ‚abszych gaÅ‚Ä™zi do duÅ¼ego drzewa, aby uzyskaÄ‡ sekwencjÄ™ najlepszych poddrzew, jako funkcjÄ™ parametru $\alpha.$
    
3. UÅ¼yj K-krotnej walidacji krzyÅ¼owej do wyboru parametru $\alpha$. Podziel obserwacje treningowe na K foldÃ³w. Dla kaÅ¼dego $k = 1, \ldots, K$:
    
    1. PowtÃ³rz kroki 1 i 2 na wszystkich foldach poza k-tym foldem danych treningowych.
    2. OceÅ„ Å›redni bÅ‚Ä…d kwadratu predykcji na danych z k-tego foldu, jako funkcjÄ™ parametru $\alpha$.
    
    UÅ›rednij wyniki dla kaÅ¼dej wartoÅ›ci $\alpha$ i wybierz $\alpha$, ktÃ³re minimalizuje Å›redni bÅ‚Ä…d.
    
4. ZwrÃ³Ä‡ poddrzewo z kroku 2, ktÃ³re odpowiada wybranej wartoÅ›ci $\alpha$.

## Boosting 

Boosting polega na wykorzystaniu wielu sÅ‚abych klasyfikatorÃ³w, z ktÃ³rych kaÅ¼dy kolejny jest trenowany w taki sposÃ³b, aby kÅ‚aÅ›Ä‡ wiÄ™kszy nacisk na prÃ³bki, ktÃ³re zostaÅ‚y bÅ‚Ä™dnie sklasyfikowane przez poprzedni model.

Jest to ogÃ³lne podejÅ›cie, moÅ¼na boostowaÄ‡ rÃ³Å¼ne metody. SzczegÃ³lnym, popularnym, algorytmem dobierania wag modeli i modyfikowania wag przykÅ‚adÃ³w jest Ada Boost. Algorytm wyglÄ…da nastÄ™pujÄ…co:

1.  Na poczÄ…tku kaÅ¼dy przykÅ‚ad w zbiorze treningowym ma przypisanÄ… wagÄ™Â  $w_i = \frac{1}{n}$ , gdzieÂ  $n$Â  to liczba przykÅ‚adÃ³w.
2. Powtarzaj dla krokÃ³w $m = 1, \dots, M$
	- **Dopasowanie modelu:** Trenujemy nowy model do danych, ale z uwzglÄ™dnieniem wag przykÅ‚adÃ³w. KaÅ¼dy nowy model skupia siÄ™ na bÅ‚Ä™dach popeÅ‚nionych przez poprzednie modele, co oznacza, Å¼e przykÅ‚ady, ktÃ³re zostaÅ‚y Åºle sklasyfikowane, otrzymujÄ… wyÅ¼sze wagi.
	
	- **Obliczenie wag:** Po dopasowaniu modeluÂ  $h_m$ , obliczamy wagÄ™ tego modeluÂ  $\alpha_m$ , ktÃ³ra jest zaleÅ¼na od jego dokÅ‚adnoÅ›ci. WzÃ³r na wagÄ™ to: $$\alpha_m = \log\left(\frac{1 - \text{err}_m}{\text{err}_m}\right)$$gdzieÂ  $\text{err}_m$Â  to bÅ‚Ä…d modeluÂ  $h_m$.
	
	- **Uaktualnianie wag przykÅ‚adÃ³w:** Wagi przykÅ‚adÃ³w sÄ… aktualizowane w taki sposÃ³b, Å¼e dla Åºle sklasyfikowanych przykÅ‚adÃ³wÂ  $w_i$Â  sÄ… zwiÄ™kszane. Aktualizacja wag jest zaleÅ¼na od dokÅ‚adnoÅ›ci danego modelu: $$w_i := w_i \exp(\alpha_m 1\{y_i \neq h_m(x_i)\})$$gdzieÂ  $\exp(\alpha_m)$Â  sprawia, Å¼e przykÅ‚ady bÅ‚Ä™dnie sklasyfikowane przez model otrzymujÄ… wyÅ¼sze wagi.

3. **Ostateczna predykcja:**
	Modele te sÄ… nastÄ™pnie uÅ›redniane z wagami i ostateczne wyjÅ›cie z modelu caÅ‚oÅ›ciowego ma postaÄ‡: $$h(x) = \text{sign}\left(\sum_m \alpha_m h_m(x)\right)$$
## Bagging 

**PowtÃ³rzenie, Bootstrap:**  
Bootstrap to technika losowania prÃ³bek ze zbioru danych _ze zwracaniem_. Oznacza to, Å¼e kaÅ¼dy element zbioru moÅ¼e zostaÄ‡ wybrany wiÄ™cej niÅ¼ raz. Zbiory bootstrapowane majÄ… ten sam rozmiar co zbiÃ³r oryginalny, ale mogÄ… zawieraÄ‡ powtÃ³rzenia.

Bagging jest metodÄ…, ktÃ³ra wykorzystuje uÅ›rednianie predykcji wielu modeli uczonych na rÃ³Å¼nych prÃ³bach bootstrapowych. GÅ‚Ã³wnym celem jest zmniejszenie wariancji, szczegÃ³lnie w przypadku modeli, ktÃ³re majÄ… tendencjÄ™ do nadmiernego dopasowania (overfitting), jak drzewa decyzyjne.

1. **Budowa modelu:**
    - Na poczÄ…tku losujemy $B$ prÃ³bek ze zbioru danych z uÅ¼yciem bootstrapu (czyli z powtÃ³rzeniami).
    - Dla kaÅ¼dej prÃ³bki budujemy osobne drzewo decyzyjne (lub inny model), ktÃ³re bÄ™dzie miaÅ‚o _maÅ‚e obciÄ…Å¼enie_ (bo jest uczone na czÄ™Å›ci zbioru) i duÅ¼Ä… _wariancjÄ™_ (poniewaÅ¼ uÅ¼ywa tylko czÄ™Å›ci danych).
    
1. **UÅ›rednianie wynikÃ³w:**
    - Po wytrenowaniu $B$ modeli na rÃ³Å¼nych prÃ³bach, wyniki sÄ… uÅ›redniane (w przypadku regresji) lub agregowane za pomocÄ… gÅ‚osowania wiÄ™kszoÅ›ciowego (w przypadku klasyfikacji).
    - WzÃ³r dla ostatecznej predykcji w baggingu to:$$h(x) = \frac{1}{B} \sum_{b=1}^{B} h_b(x)$$gdzie $h_b(x)$ jest predykcjÄ… modelu $b$, wytrenowanego na bootstrapowanej prÃ³bce.
    
3. **Redukcja wariancji:**
    - PoniewaÅ¼ kaÅ¼dy z modelÃ³w jest uczony na rÃ³Å¼nych prÃ³bkach, majÄ… one rÃ³Å¼ne bÅ‚Ä™dy (wariancjÄ™), ale po uÅ›rednieniu, wariancja caÅ‚kowita modelu jest mniejsza, a dokÅ‚adnoÅ›Ä‡ poprawia siÄ™, szczegÃ³lnie w przypadku drzew decyzyjnych, ktÃ³re majÄ… tendencjÄ™ do przeuczenia.
## Random Forests

- Pojedyncze drzewo decyzyjne jest podatne na szum w zbiorze uczÄ…cym, co moÅ¼e prowadziÄ‡ do przeuczenia (overfitting).
- Mimo zastosowania technik takich jak przycinanie, pojedyncze drzewo moÅ¼e nadal byÄ‡ niewystarczajÄ…ce w modelowaniu bardziej zÅ‚oÅ¼onych zaleÅ¼noÅ›ci.

**PomysÅ‚ na PoprawÄ™ â€“ Lasy Losowe:**
- Aby rozwiÄ…zaÄ‡ problem przeuczenia, stosuje siÄ™ podejÅ›cie, w ktÃ³rym budowane sÄ… **wielokrotnie rÃ³Å¼ne drzewa**, rÃ³Å¼niÄ…ce siÄ™ miÄ™dzy sobÄ…, a ich wyniki sÄ… uÅ›redniane.
- Pierwsza randomizacja polega na zastosowaniu **podzbiorÃ³w zbioru uczÄ…cego**:
    - Losujemy prÃ³bki z oryginalnego zbioru z **bootstrapem**, co daje rÃ³Å¼ne zbiory treningowe dla kaÅ¼dego drzewa.
    - PrÃ³bki, ktÃ³re nie pojawiÅ‚y siÄ™ w danym bootstrapie, mogÄ… zostaÄ‡ uÅ¼yte do **walidacji krzyÅ¼owej** (CV), co pozwala na lepszÄ… ocenÄ™ modelu.
- Druga randomizacja polega na zastosowaniu **podzbioru cech**:
    - Przy rozwaÅ¼aniu podziaÅ‚u w kaÅ¼dym wÄ™Åºle drzewa, zamiast wszystkich cech, wybieramy losowo $m$ cech spoÅ›rÃ³d wszystkich $p$ dostÄ™pnych cech, gdzie $mâ‰ªp$. CzÄ™sto przyjmuje siÄ™, Å¼e $m = \sqrt{p}m$â€‹.
    - Taka technika sprawia, Å¼e drzewa w lesie losowym rÃ³Å¼niÄ… siÄ™ od siebie, co redukuje ich **wzajemnÄ… korelacjÄ™**, a tym samym zmniejsza ryzyko przeuczenia.
    - Dodatkowo, dziÄ™ki temu uwalniamy siÄ™ od wpÅ‚ywu najsilniejszych predyktorÃ³w - mogÄ… one nie wpaÅ›Ä‡ do zbioru $m$ rozwaÅ¼anych


# Pytania do wykÅ‚adu
1. JakÄ… postaÄ‡ ma podziaÅ‚ przestrzeni cech?
2. Co poprawia duÅ¼a liczba drzew w lesie losowym?
3. Czym siÄ™ rÃ³Å¼niÄ… metody post-prune od rule post-prune?