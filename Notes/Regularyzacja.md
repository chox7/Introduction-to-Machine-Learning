Przypomnienie:

Metoda najmniejszych kwadrat贸w = minimalizacja RSS

$$
RSS = \sum_{i=1}^n\left(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\right)^2
$$

## Regresja grzbietowa (ridge regression)

$$
\sum_{i=1}^n\left(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda\sum_{j=1}^p\beta_j^2 = RSS+\lambda\sum_{j=1}^p\beta_j^2
$$

- dodatkowa kara cigajca
- $\lambda$ jest parametrem sterujcym metody, kontroluje jak silnie parametry s cigane do 0.
    - dla $\lambda=0$ zwyka metoda najmniejszych kwadrat贸w
    - dla $\lambda \to \infty$ wsp贸czynniki $\beta_i$ kurcz si do 0 (**ale nigdy nie bd r贸wne 0**)
- nie cigamy wyrazu wolnego $\beta_0$ - jest to predykcja redniej

Standardowe oszacowania wsp贸czynnik贸w metod najmniejszych kwadrat贸w s r贸wnowa偶ne pod wzgldem skali, niezale偶nie od sposobu skalowania j-tego predyktora, $X_j\hat{\beta}_j$ pozostanie taki sam. W przeciwiestwie do tego, oszacowania wsp贸czynnika regresji grzbietowej mog si zmienia podczas mno偶enia danego predyktora przez sta (przy zmianie skali).

<aside>
 Przed zastosowaniem regresji grzbietowej, standaryzuje si predyktory tak aby byy w tej samej skali.

</aside>

### Regresja grzbietowa a metoda najmniejszych kwadrat贸w

Przewaga regresji grzbietowej nad regresj najmniejszych kwadrat贸w jest zakorzeniona w odchyleniu od wariancji. Wraz ze wzrostem $位$ zmniejsza si elastyczno dopasowania regresji grzbietowej, co prowadzi do zmniejszenia wariancji, ale zwikszenia obci偶enia. 

- Gdy liczba zmiennych p jest bliska liczbie obserwacji n to metoda najmniejszych kwadrat贸w ma du偶 wariancje
- Gdy $p>n$ to metoda najmniejszych kwadrat贸w przestaje dziaa (brak jednoznacznoci estymacji), podczas gdy regresja grzbietowa mo偶e efektywnie zmniejszy wariancj kosztem niewielkiego wzrostu obci偶enia i wybra najlepszy model spor贸d wielu dajcych $RSS=0$
- Regresja grzbietowa najlepiej sprawdza si w sytuacjach, gdzie estymaty najmniejszych kwadrat贸w maj wysok wariancj.
- Regresja grzbietowa  mo偶e by u偶yto do wyboru modelu (pomijamy tu predyktory, dla kt贸rych wsp贸czynnik $\beta_j$ jest may) i ma znaczn przewag obliczeniow nad selekcj najlepszych podzbior贸w, kt贸ra wymaga przeszukania  $2^p$ modeli.

## Lasso regression

$$
\sum_{i=1}^n\left(y_i-\beta_0-\sum_{j=1}^p\beta_jx_{ij}\right)^2 + \lambda\sum_{j=1}^p|\beta_j| = RSS+\lambda\sum_{j=1}^p|\beta_j|
$$

- dla dostatecznie du偶ych wartoci $\lambda$ estymacje niekt贸rych parametr贸w $\beta_j$ przyjmuj wartoci r贸wne 0, jednoczenie zadajc selekcj predyktor贸w

### Zjawisko selekcji predyktor贸w

Metoda lasso jest r贸wnowa偶na problemowi minimalizacji po wektorach $\beta$ wyra偶enia:

$$
\sum_{i=1}^n\left(y_i-\beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 \text{przy warunku } \sum_{j=1}^p|\beta_j|\le s
$$

Metoda grzbietowej regresji jest r贸wnowa偶na problemowi minimalizacji po wektorach  $\beta$ wyra偶enia:

$$
\sum_{i=1}^n\left(y_i-\beta_0 - \sum_{j=1}^p\beta_jx_{ij}\right)^2 \text{przy warunku } \sum_{j=1}^p\beta_j^2\le s
$$

Dla ka偶dego $\lambda \ge 0$ istnieje takie $s \ge 0$ speniajce powy偶sze warunki.

![[hierarchiczna.png]]

### Por贸wnanie regresji grzbietowej z lasso

- 呕adna z metod nie jest lepsza od drugiej we wszystkich sytuacjach.
- Lasso dziaa lepiej w sytuacjach, gdy zmienna objaniana Y istotnie zale偶y tylko od maej liczby predyktor贸w.
- Regresja grzbietowa dziaa lepiej gdy Y zale偶y od du偶ej liczby predyktor贸w o wsp贸czynnikach w przybli偶eniu podobnego rozmiaru.
- Liczba istotnych predyktor贸w nigdy nie jest znana z g贸ry - trzeba stosowa walidacj krzy偶ow do wyboru modelu.
- W sytuacji gdy estymacje z metody najmniejszych kwadrat贸w majdu偶 wariancj to zar贸wno lasso, jaki regresja grzbietowa zmniejszaj wariancjpredykcji kosztem zwikszenia obci偶enia.
- Metoda lasso, w odr贸偶nieniu od regresji grzbietowej, pozwala dokonywaselekcji istotnych predyktor贸w.

# Metody redukcji wymiaru

Redukcja wymiaru polega na zastpieniu oryginalnych predyktor贸w $X_1, X_2, \dots, X_p$ linowymi kombinacjami $Z_1, Z_2, \dots, Z_M$, gdzie $M \le p$ oraz dla $1 \le m\le M$:

$$
Z_m=\sum_{j=1}^p \phi_{jm}X_j
$$

gdzie $\phi_{1m},  \phi_{2m}, \dots,  \phi_{pm}$ s pewnymi staymi (parametrami) metody.

Stosujc metod najmniejszych kwadrat贸w, znajdujemy parametry $\theta_0, \theta_1, \dots, \theta_M$ modelu regresji liniowej:

$$
y_i = \theta_0  +\sum_{m=1}^M\theta_mz_{im} + \epsilon_i, \ \ \ \ \ \ \ \ \ \ i = 1, \dots, n
$$

- Wymiar zosta zredukowany z $p+1$ do $M+1$
- Przy odpowiednim doborze staych $\phi_{jm}$ taka redukcja wymiaru czsto poprawia jako predykcji