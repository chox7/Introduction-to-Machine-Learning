- [SVM - THE MATH YOU SHOULD KNOW](https://www.youtube.com/watch?v=05VABNfa1ds&ab_channel=CodeEmporium)



- Na zbi√≥r uczƒÖcy patrzymy jako na zbi√≥r punkt√≥w w przestrzeni cech. Konstrukcja klasyfikatora polega na znalezieniu hiperpowierzchni mo≈ºliwie dobrze separujƒÖcej elementy oznaczone r√≥≈ºnymi etykietkami. 
- Na potrzeby SVM przyjmujemy: 
$$ h_{w,b}(x) = g(w^Tx + b) = \lbrace \begin{array} {rcl} 1 & dla & w^{T}x+b \ge 0 \\ -1 & dla & w^{T}x+b < 0 \\ \end{array} $$
## Marginesy funkcjonalne i geometryczne
### Marginesy funkcjonalne

Dla przyk≈Çadu ze zbioru uczƒÖcego $(x^{(j)},y^{(j)})$ definiujemy margines funkcjonalny jako: $$\qquad

{\hat{\gamma}}\;\; ^{(j)} = y^{(j)}(w^{T}x^{(j)} + b)

$$Zauwa≈ºmy, ≈ºe:
* dla $y^{(j)}=1$ mamy du≈ºy margines funkcjonalny je≈õli $w^{T}x^{(j)} +b$ jest du≈ºƒÖ dodatniƒÖ liczbƒÖ.
* dla $y^{(j)}=-1$ mamy du≈ºy margines funkcjonalny je≈õli $w^{T}x^{(j)} +b$ jest du≈ºƒÖ ujemnƒÖ liczbƒÖ.
* Ponadto je≈õli klasyfikacja jest prawid≈Çowa to margines funkcjonalny jest dodatni.

**Problem skalowania** $w$ i $b$ - margines funkcjonalny mo≈ºna sztucznie zwiƒôkszyƒá, mno≈ºƒÖc  $w$ i $b$ przez tƒô samƒÖ, du≈ºƒÖ liczbƒô $k$, poniewa≈º:$$ \hat{\gamma}^{(j)} = y^{(j)} (k \cdot w^T x^{(j)} + k \cdot b) = k \cdot \hat{\gamma}^{(j)}$$ Aby to zneutralizowaƒá , normalizujemy $w$ i $b$: $$  

w \leftarrow \frac{w}{\|w\|}, \quad b \leftarrow \frac{b}{\|w\|}.$$
Margines funkcjonalny dla ca≈Çego zbioru uczƒÖcego definiuje siƒô jako najmniejszy margines funkcjonalny spo≈õr√≥d wszystkich przyk≈Çad√≥w w zbiorze. Formalnie: $$\hat{\gamma} = \min_{j=1,\dots,m} \hat{\gamma}^{(j)} = \min_{j=1,\dots,m} \left( y^{(j)} (w^T x^{(j)} + b) \right)$$

### Marginesy geometryczne

Margines geometryczny¬†$\gamma^{(j)}$¬† dla pojedynczego punktu¬† $x^{(j)}$¬† to najmniejsza odleg≈Ço≈õƒá od punktu do hiperpowierzchni rozdzielajƒÖcej¬† $w^T x + b = 0$ . Aby to obliczyƒá, stosujemy nastƒôpujƒÖce kroki:

Hiperpowierzchnia separujƒÖca jest zdefiniowana r√≥wnaniem: $$  
w^T x + b = 0$$ 
Punkt¬† $x^{(j)}$¬† przesuwamy w kierunku wektora normalnego¬† $\frac{w}{\|w\|}$ , a≈º znajdzie siƒô na hiperpowierzchni. Przesuniƒôcie punktu opisuje siƒô jako: $$   

x^{(j)} - \gamma^{(j)} \frac{w}{\|w\|}$$
Je≈õli przesuniƒôty punkt le≈ºy na hiperpowierzchni, to spe≈Çnia r√≥wnanie: $$w^T \left( x^{(j)} - \gamma^{(j)} \frac{w}{\|w\|} \right) + b = 0$$
RozwijajƒÖc r√≥wnanie: $$  

w^T x^{(j)} - \gamma^{(j)} w^T \frac{w}{\|w\|} + b = 0$$ $$  

w^T x^{(j)} + b = \gamma^{(j)} \|w\|$$ $$  

\gamma^{(j)} = \frac{w^T x^{(j)} + b}{\|w\|}$$
Dla uwzglƒôdnienia obu klas $( y^{(j)} = \pm 1 )$, mo≈ºemy zapisaƒá og√≥lnƒÖ formu≈Çƒô: $$   

\gamma^{(j)} = y^{(j)} \frac{w^T x^{(j)} + b}{\|w\|}$$
Margines geometryczny dla ca≈Çego zbioru uczƒÖcego definiujemy jako najmniejszy margines geometryczny spo≈õr√≥d wszystkich przyk≈Çad√≥w w zbiorze: $$\gamma = \min_{j=1,\dots,m} \gamma^{(j)}$$
### ZwiƒÖzek marginesu geometrycznego z marginesem funkcjonalnym

Je≈õli wektor¬† $w$¬† jest znormalizowany $( \|w\| = 1 )$, to margines funkcjonalny i margines geometryczny sƒÖ r√≥wne, poniewa≈º:
$$\gamma^{(j)} = \frac{\hat{\gamma}^{(j)}}{\|w\|} = \hat{\gamma}^{(j)}$$

## Klasyfikator SVM 
 
Podstawowa wersja SVM (Support Vector Machine) dla problem√≥w liniowo separowalnych. Celem jest znalezienie hiperpowierzchni, kt√≥ra **maksymalizuje minimalny margines geometryczny**, czyli:  $$   

\max_{w, b} \min_{j \in \{1, \dots, m\}} \frac{y^{(j)} \left(w^T x^{(j)} + b\right)}{\|w\|}$$
DƒÖ≈ºymy do tego, aby punkty by≈Çy jak najdalej od hiperpowierzchni decyzyjnej, gwarantujƒÖc, ≈ºe klasy sƒÖ dobrze separowane.

### Wersja 1: Maksymalizacja marginesu geometrycznego

Pierwotny problem optymalizacyjny mo≈ºna sformu≈Çowaƒá jako: $$  

\max_{w, b} \, \gamma

\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq \gamma, \, j=1,\dots,m, \quad \|w\| = 1$$
‚Ä¢ Maksymalizujemy¬† $\gamma$ , minimalny margines geometryczny.
‚Ä¢ Warunek¬† $\|w\| = 1$¬† zapewnia, ≈ºe margines geometryczny jest r√≥wny funkcjonalnemu.
‚Ä¢ Problem nie jest wypuk≈Çy, poniewa≈º ograniczenie¬†$\|w\| = 1$¬† tworzy sferƒô, a nie przestrze≈Ñ liniowƒÖ.

### Wersja 2: Przeformu≈Çowanie problemu

Rozwa≈ºajƒÖc zale≈ºno≈õƒá miƒôdzy marginesami¬† $\gamma = \frac{\hat{\gamma}}{\|w\|}$ , mo≈ºemy sformu≈Çowaƒá: $$  

\max_{w, b} \, \frac{\hat{\gamma}(w, b)}{\|w\|}

\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq \hat{\gamma}, \, j=1,\dots,m.$$
- Problem staje siƒô bardziej wygodny, ale funkcja celu¬† $\frac{\hat{\gamma}}{\|w\|}$¬† jest wciƒÖ≈º trudna do optymalizacji.
### Wersja 3: Skalowanie marginesu funkcjonalnego

Dziƒôki mo≈ºliwo≈õci skalowania¬† $w$¬† i¬† $b$ , przyjmujemy¬† $\hat{\gamma} = 1$ . Po tej normalizacji problem sprowadza siƒô do minimalizacji normy¬† $w$ , co jest r√≥wnowa≈ºne maksymalizacji marginesu geometrycznego: $$   

\min_{w, b} \, \|w\|^2

\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq 1, \, j=1,\dots,m$$
- Funkcja celu¬† $\|w\|^2$¬† jest wypuk≈Ça.
- Warunki¬† $y^{(j)} \left( w^T x^{(j)} + b \right) \geq 1$¬† sƒÖ liniowe, co sprawia, ≈ºe problem jest wypuk≈Çy z wiƒôzami.


## Mno≈ºniki Lagrange'a

Metoda mno≈ºnik√≥w Lagrange'a to uniwersalna technika rozwiƒÖzywania problem√≥w optymalizacyjnych z wiƒôzami. Mo≈ºna jƒÖ przedstawiƒá nastƒôpujƒÖco:

- Problem optymalizacyjny: $$   

\min_{w} f(w) \quad \text{p.w.:} \quad h_i(w) = 0, \; i = 1, \ldots, l$$
- Lagrangian: $$  

\mathcal{L}(w, \beta) = f(w) + \sum_{i=1}^l \beta_i h_i(w)$$ gdzie $\beta_i$ to mno≈ºniki Lagrange‚Äôa.

- RozwiƒÖzywanie problemu:
	Znalezienie ekstremum polega na spe≈Çnieniu warunk√≥w: $$  

\frac{\partial \mathcal{L}}{\partial w_i} = 0 \quad \text{i} \quad \frac{\partial \mathcal{L}}{\partial \beta_i} = 0$$
### Wiƒôzy w postaci nier√≥wno≈õci

- Metodƒô mo≈ºna rozszerzyƒá na problemy z wiƒôzami nier√≥wno≈õciowymi: $$   

\min_{w} f(w) \quad \text{p.w.:} \quad g_i(w) \leq 0, \; h_i(w) = 0$$
- Uog√≥lniony lagrangian przyjmuje postaƒá: $$   

\mathcal{L}(w, \alpha, \beta) = f(w) + \sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^l \beta_i h_i(w)$$ gdzie $\alpha_i \geq 0$.


### Problem pierwotny i dualny

- **Definicja problemu pierwotnego**: $$ \theta _{p}(w) = \max _{\alpha ,\beta :\alpha _{i} \ge 0} \mathcal {L}(w,\alpha ,\beta )$$$$ p^{*} = \min _{w} \theta _{p}(w) = \min _{w} \max _{\alpha ,\beta : \alpha _{i }\ge 0 } \mathcal {L}(w,\alpha ,\beta )$$
- **Definicja problemu dualnego**: $$\theta_d(\alpha, \beta) = \min_{w} \mathcal{L}(w, \alpha, \beta)$$ $$   

d^{*} = \max _{\alpha ,\beta : \alpha

_{i}\ge 0} \theta _{d}(\alpha, \beta) = \max _{\alpha ,\beta : \alpha _{i}\ge 0} \min _{w} \mathcal {L}(w,\alpha ,\beta )$$
- **Relacja miƒôdzy problemem pierwotnym i dualnym**: $$  

d^* \leq p^* $$

### Warunki Karusha-Kuhna-Tuckera (KKT)

R√≥wno≈õƒá $d^* = p^*$ zachodzi pod warunkami:

‚Ä¢ $f(w)$ i $g_i(w)$ sƒÖ wypuk≈Çe,
‚Ä¢ $h_i(w)$ sƒÖ afiniczne $(h_i(w) = a_i^T w + b_i)$,
‚Ä¢ Istnieje w, dla kt√≥rego $g_i(w) < 0$.


¬†Pod tymi warunkami istniejƒÖ¬†$ùë§‚àó$, $ùõº‚àó$, $ùõΩ‚àó$, dla kt√≥rych¬†$p^{*}=d^{*}=\mathcal {L} (w^{*},\alpha ^{*},\beta ^{*})$. Ponadto¬†¬†$ùë§‚àó$, $ùõº‚àó$, $ùõΩ‚àó$ spe≈ÇniajƒÖ:

1. $\frac{\partial \mathcal{L}}{\partial w_i} = 0$,
2. $\frac{\partial \mathcal{L}}{\partial \beta_i} = 0$,
3. $\alpha_i^* g_i(w^*) = 0$,
4. $g_i(w) \leq 0, \; \alpha_i \geq 0$.

Prawdziwe jest te≈º twierdzenie odwrotne. Je≈õli jakie≈õ parametry¬†ùë§‚àó,ùõº‚àó,ùõΩ‚àó¬†spe≈ÇniajƒÖ warunki KKT to sƒÖ te≈º rozwiƒÖzaniem problemu pierwotnego i dualnego.

## Klasyfikator SVM a Lagrange

Problem optymalizacyjny klasyfikatora SVM wyrazili≈õmy nastƒôpujƒÖco: $$   

\min_{w, b} \frac{1}{2} \, \|w\|^2

\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq 1, \, j=1,\dots,m$$
Przekszta≈ÇcajƒÖc do postaci pasujƒÖcej do formalizmu uog√≥lnionej metody Lagrange'a: $$   

\min_{w, b} \frac{1}{2} \, \|w\|^2

\quad \text{p.w.} \quad g_{j}(w,b) = 1 - y^{(j)}(w^{T}x^{(j)}+b) \le 0, \quad j= 1, \dots ,m$$
Lagrangian dla tego problemu wyglƒÖda tak:
$$\mathcal {L}(w,b,\alpha ) = \frac{1}{2}||w||^{2} + \sum _{j=1}^{m} \alpha _{j}g_j(w,b)=$$
$$= \frac{1}{2}||w||^{2} + \sum _{j=1}^{m} \alpha _{j}\left[1 - y^{(j)}(w^{T}x^{(j)}+b) \right]=$$$$=\frac{1}{2}||w||^{2} - \sum _{j=1}^{m} \alpha _{j}\left[ y^{(j)}(w^{T}x^{(j)}+b) -1\right]$$
### Przej≈õcie do postaci dualnej
$$\mathcal{L}(w, b, \alpha) = \frac{1}{2} \|w\|^2 - \sum_{j=1}^{m} \alpha_j \left[ y^{(j)} \left(w^T x^{(j)} + b\right) - 1 \right]$$
**Krok 1: Minimalizacja wzglƒôdem $w$ i $b$**
- Pochodna wzglƒôdem $w$: $$   

\nabla_w \mathcal{L}(w, b, \alpha) = w - \sum_{j=1}^{m} \alpha_j y^{(j)} x^{(j)} = 0 $$ StƒÖd, po przekszta≈Çceniu: $$   

w^* = \sum_{j=1}^{m} \alpha_j y^{(j)} x^{(j)} $$
- Pochodna wzglƒôdem $b$: $$   

\nabla_b \mathcal{L}(w, b, \alpha) = \sum_{j=1}^{m} \alpha_j y^{(j)} = 0 $$
**Krok 2: Podstawienie $w^*$ do Lagrangiana**
- Norma $w^*$ jest r√≥wna: $$  

\|w^*\|^2 = \left\| \sum_{j=1}^{m} \alpha_j y^{(j)} x^{(j)} \right\|^2 = \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle $$
- Po podstawieniu do funkcji Lagrangiana, uzyskujemy: $$
\mathcal{L}(w^*, b, \alpha) = \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{j=1}^{m} \alpha_j \left[ y^{(j)} \left( \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x^{(j)} \rangle + b \right) - 1 \right] = $$$$= \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{j=1}^{m} \alpha_j y^{(j)} \left( \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x^{(j)} \rangle + b \right) + \sum_{j=1}^{m} \alpha_j = $$ $$= \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{j=1}^{m} \alpha_j y^{(j)} \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x^{(j)} \rangle - b\sum_{j=1}^{m} \alpha_j y^{(j)}  + \sum_{j=1}^{m} \alpha_j = $$ $$ = \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - b\sum_{j=1}^{m} \alpha_j y^{(j)} + \sum_{j=1}^{m} \alpha_j =$$ $$ = -\frac{1}{2}\sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - b\sum_{j=1}^{m} \alpha_j y^{(j)}  + \sum_{j=1}^{m} \alpha_j$$
- Z warunku KTT wynika $\sum_{j=1}^{m} \alpha_j y^{(j)} = 0$, zatem: $$ \mathcal{L}(w^*, b, \alpha) = \sum_{j=1}^{m} \alpha_j -\frac{1}{2}\sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle$$
**Krok 3: Dualny problem optymalizacyjny**
Funkcja $\mathcal{L}(w^*, b, \alpha)$ jest teraz funkcjƒÖ tylko w zmiennych $\alpha_j$, a naszym celem jest jej zmaksymalizowanie, przy zachowaniu odpowiednich ogranicze≈Ñ. Ostateczny dualny problem optymalizacyjny zapisujemy jako: $$\max_{\alpha} \left( \sum_{j=1}^{m} \alpha_j - \frac{1}{2} \sum_{i,j=1}^{m} y^{(i)} y^{(j)} \alpha_i \alpha_j \langle x^{(i)}, x^{(j)} \rangle \right)$$
Pod warunkiem:
- $\alpha_j \geq 0, \quad j = 1, \dots, m$
- $\sum_{j=1}^{m} \alpha_j y^{(j)} = 0$

Spe≈Çnione sƒÖ warunki KKT, zatem rozwiƒÖzanie tego problemu dualnego jest te≈º rozwiƒÖzaniem naszego problemu pierwotnego.


Po rozwiƒÖzaniu problemu dualnego SVM otrzymujemy optymalne warto≈õci mno≈ºnik√≥w Lagrange‚Äôa $\alpha^*_j$. Warto≈õƒá $\alpha^*_j > 0$ wskazuje, ≈ºe odpowiadajƒÖcy mu punkt $(x^{(j)}, y^{(j)})$ jest wektorem no≈õnym. **(wektory no≈õne - przyk≈Çady po≈Ço≈ºone najbli≈ºej hiperpowierzchni decyzyjnej)**.

Zatem podstawiajƒÖc $\alpha^*_j$: $$  

w^* = \sum_{j=1}^{m} \alpha^*_j y^{(j)} x^{(j)}$$ $$   

b^* = -\frac{1}{2} \left( \max_{j : y^{(j)} = -1} w^* \cdot x^{(j)} + \min_{j : y^{(j)} = 1} w^* \cdot x^{(j)} \right)$$

# Pytania do wyk≈Çadu
1. Algorytm SVM polega na ___ margines√≥w geometrycznych.
2. Czy margines geometryczny danego przyk≈Çadu mo≈ºna zmieniƒá przez mno≈ºenie przez skalar?
3. Dodatnia warto≈õƒá marginesu funkcjonalnego ≈õwiadczy o ___ .
4. Margines geometryczny dla zbioru uczƒÖcego to: ___ ¬†z margines√≥w otrzymanych dla ka≈ºdego z przyk≈Çad√≥w.
5. Co gwarantuje warunek: $$ y^{(j)}(w^{T}x^{(j)} + b) \geq 1,  j = 1,...,m$$ w trzeciej wersji problemu optymalizacyjnego SVM? 
6. Problem pierwotny dla uog√≥lnionego Lagrangianu polega na najpierw¬† ___ wzglƒôdem ___ , a nastƒôpnie na ___ wzglƒôdem ___ .
7. Czy parametry uog√≥lnionego Lagrangianu spe≈ÇniajƒÖce warunki KKT sƒÖ rozwiƒÖzaniem problemu pierwotnego?
8. Czym sƒÖ wektory no≈õne?
9. Czy w technice SVM mapowanie wej≈õcia do wiƒôcej wymiarowej przestrzeni wykonujemy w spos√≥b jawny?
10. Co robi funkcja jƒÖdrowa? JakƒÖ warto≈õƒá zwraca dla wektor√≥w, kt√≥re sƒÖ do siebie podobne? Czego ta funkcja jest miarƒÖ?
11. Czy warunek: "je≈õli mamy jakie≈õ mapowanie¬†$\phi$¬†i zwiƒÖzane z nim jƒÖdro¬†$K$¬†to macierz jƒÖdra jest symetryczna i dodatnio okre≈õlona" jest warunkiem wystarczajƒÖcym aby¬†$K$¬†by≈Ça jƒÖdrem?
12. Czy metodƒô optymalizacji osiowej mo≈ºna zastosowaƒá wprost do problemu SVM?