- [SVM - THE MATH YOU SHOULD KNOW](https://www.youtube.com/watch?v=05VABNfa1ds&ab_channel=CodeEmporium)



- Na zbiÃ³r uczÄ…cy patrzymy jako na zbiÃ³r punktÃ³w w przestrzeni cech. Konstrukcja klasyfikatora polega na znalezieniu hiperpowierzchni moÅ¼liwie dobrze separujÄ…cej elementy oznaczone rÃ³Å¼nymi etykietkami. 
- Na potrzeby SVM przyjmujemy: 
$$ \begin{equation} 
h_{w,b}(x) = g(w^Tx + b) = \lbrace \begin{array} {rcl} 1 & dla & w^{T}x+b \ge 0 \\ -1 & dla & w^{T}x+b < 0 \\ \end{array}
\end{equation} $$
## Marginesy funkcjonalne i geometryczne
### Marginesy funkcjonalne

Dla przykÅ‚adu ze zbioru uczÄ…cego $(x^{(j)},y^{(j)})$ definiujemy margines funkcjonalny jako: $$\begin{equation} 
\qquad
{\hat{\gamma}}\;\; ^{(j)} = y^{(j)}(w^{T}x^{(j)} + b)
\end{equation}$$ZauwaÅ¼my, Å¼e:
* dla $y^{(j)}=1$ mamy duÅ¼y margines funkcjonalny jeÅ›li $w^{T}x^{(j)} +b$ jest duÅ¼Ä… dodatniÄ… liczbÄ….
* dla $y^{(j)}=-1$ mamy duÅ¼y margines funkcjonalny jeÅ›li $w^{T}x^{(j)} +b$ jest duÅ¼Ä… ujemnÄ… liczbÄ….
* Ponadto jeÅ›li klasyfikacja jest prawidÅ‚owa to margines funkcjonalny jest dodatni.

**Problem skalowania** $w$ i $b$ - margines funkcjonalny moÅ¼na sztucznie zwiÄ™kszyÄ‡, mnoÅ¼Ä…c  $w$ i $b$ przez tÄ™ samÄ…, duÅ¼Ä… liczbÄ™ $k$, poniewaÅ¼:
$$\begin{equation}
\hat{\gamma}^{(j)} = y^{(j)} (k \cdot w^T x^{(j)} + k \cdot b) = k \cdot \hat{\gamma}^{(j)} 
\end{equation}$$ 
Aby to zneutralizowaÄ‡ , normalizujemy $w$ i $b$: 
$$ \begin{equation} 
w\leftarrow \frac{w}{\|w\|}, \quad b \leftarrow \frac{b}{\|w\|}
\end{equation}$$

Margines funkcjonalny dla caÅ‚ego zbioru uczÄ…cego definiuje siÄ™ jako najmniejszy margines funkcjonalny spoÅ›rÃ³d wszystkich przykÅ‚adÃ³w w zbiorze. Formalnie: $$\hat{\gamma} = \min_{j=1,\dots,m} \hat{\gamma}^{(j)} = \min_{j=1,\dots,m} \left( y^{(j)} (w^T x^{(j)} + b) \right)$$\begin{equation}

### Marginesy geometryczne

Margines geometrycznyÂ $\gamma^{(j)}$Â  dla pojedynczego punktuÂ  $x^{(j)}$Â  to najmniejsza odlegÅ‚oÅ›Ä‡ od punktu do hiperpowierzchni rozdzielajÄ…cejÂ  $w^T x + b = 0$ . Aby to obliczyÄ‡, stosujemy nastÄ™pujÄ…ce kroki:

Hiperpowierzchnia separujÄ…ca jest zdefiniowana rÃ³wnaniem: 
$$\begin{equation}  
w^T x + b = 0
\end{equation}$$ 
PunktÂ  $x^{(j)}$Â  przesuwamy w kierunku wektora normalnegoÂ  $\frac{w}{\|w\|}$ , aÅ¼ znajdzie siÄ™ na hiperpowierzchni. PrzesuniÄ™cie punktu opisuje siÄ™ jako: 
$$\begin{equation}
x^{(j)} - \gamma^{(j)} \frac{w}{\|w\|}
\end{equation}$$
JeÅ›li przesuniÄ™ty punkt leÅ¼y na hiperpowierzchni, to speÅ‚nia rÃ³wnanie: 
$$\begin{equation}w^T \left( x^{(j)} - \gamma^{(j)} \frac{w}{\|w\|} \right) + b = 0
\end{equation}$$
RozwijajÄ…c rÃ³wnanie: 
$$\begin{equation}
w^T x^{(j)} - \gamma^{(j)} w^T \frac{w}{\|w\|} + b = 0
\end{equation}$$
$$\begin{equation}
w^T x^{(j)} + b = \gamma^{(j)} \|w\|
\end{equation}$$ 
$$\begin{equation}
\gamma^{(j)} = \frac{w^T x^{(j)} + b}{\|w\|}
\end{equation}$$
Dla uwzglÄ™dnienia obu klas $( y^{(j)} = \pm 1 )$, moÅ¼emy zapisaÄ‡ ogÃ³lnÄ… formuÅ‚Ä™: 
$$\begin{equation}
\gamma^{(j)} = y^{(j)} \frac{w^T x^{(j)} + b}{\|w\|}
\end{equation}$$
Margines geometryczny dla caÅ‚ego zbioru uczÄ…cego definiujemy jako najmniejszy margines geometryczny spoÅ›rÃ³d wszystkich przykÅ‚adÃ³w w zbiorze: $$\gamma = \min_{j=1,\dots,m} \gamma^{(j)}$$
### ZwiÄ…zek marginesu geometrycznego z marginesem funkcjonalnym

JeÅ›li wektorÂ  $w$Â  jest znormalizowany $( \|w\| = 1 )$, to margines funkcjonalny i margines geometryczny sÄ… rÃ³wne, poniewaÅ¼:
$$\begin{equation}
\gamma^{(j)} = \frac{\hat{\gamma}^{(j)}}{\|w\|} = \hat{\gamma}^{(j)}
\end{equation}$$

## Klasyfikator SVM 
 
Podstawowa wersja SVM (Support Vector Machine) dla problemÃ³w liniowo separowalnych. Celem jest znalezienie hiperpowierzchni, ktÃ³ra **maksymalizuje minimalny margines geometryczny**, czyli:  
$$\begin{equation}
\max_{w, b} \min_{j \in \{1, \dots, m\}} \frac{y^{(j)} \left(w^T x^{(j)} + b\right)}{\|w\|}
\end{equation}$$
DÄ…Å¼ymy do tego, aby punkty byÅ‚y jak najdalej od hiperpowierzchni decyzyjnej, gwarantujÄ…c, Å¼e klasy sÄ… dobrze separowane.

### Wersja 1: Maksymalizacja marginesu geometrycznego

Pierwotny problem optymalizacyjny moÅ¼na sformuÅ‚owaÄ‡ jako: 
$$\begin{equation}
\max_{w, b} \, \gamma
\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq \gamma, \, j=1,\dots,m, \quad \|w\| = 1
\end{equation}$$
â€¢ MaksymalizujemyÂ  $\gamma$ , minimalny margines geometryczny.
â€¢ WarunekÂ  $\|w\| = 1$Â  zapewnia, Å¼e margines geometryczny jest rÃ³wny funkcjonalnemu.
â€¢ Problem nie jest wypukÅ‚y, poniewaÅ¼ ograniczenieÂ $\|w\| = 1$Â  tworzy sferÄ™, a nie przestrzeÅ„ liniowÄ….

### Wersja 2: PrzeformuÅ‚owanie problemu

RozwaÅ¼ajÄ…c zaleÅ¼noÅ›Ä‡ miÄ™dzy marginesamiÂ  $\gamma = \frac{\hat{\gamma}}{\|w\|}$ , moÅ¼emy sformuÅ‚owaÄ‡: 
$$\begin{equation}
\max_{w, b} \, \frac{\hat{\gamma}(w, b)}{\|w\|}
\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq \hat{\gamma}, \, j=1,\dots,m
\end{equation}$$
- Problem staje siÄ™ bardziej wygodny, ale funkcja celuÂ  $\frac{\hat{\gamma}}{\|w\|}$Â  jest wciÄ…Å¼ trudna do optymalizacji.
### Wersja 3: Skalowanie marginesu funkcjonalnego

DziÄ™ki moÅ¼liwoÅ›ci skalowaniaÂ  $w$Â  iÂ  $b$ , przyjmujemyÂ  $\hat{\gamma} = 1$ . Po tej normalizacji problem sprowadza siÄ™ do minimalizacji normyÂ  $w$ , co jest rÃ³wnowaÅ¼ne maksymalizacji marginesu geometrycznego: 
$$\begin{equation}
\min_{w, b} \, \|w\|^2
\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq 1, \, j=1,\dots,m
\end{equation}$$
- Funkcja celuÂ  $\|w\|^2$Â  jest wypukÅ‚a.
- WarunkiÂ  $y^{(j)} \left( w^T x^{(j)} + b \right) \geq 1$Â  sÄ… liniowe, co sprawia, Å¼e problem jest wypukÅ‚y z wiÄ™zami.


## MnoÅ¼niki Lagrange'a

Metoda mnoÅ¼nikÃ³w Lagrange'a to uniwersalna technika rozwiÄ…zywania problemÃ³w optymalizacyjnych z wiÄ™zami. MoÅ¼na jÄ… przedstawiÄ‡ nastÄ™pujÄ…co:

- Problem optymalizacyjny: 
$$\begin{equation}
\min_{w} f(w) \quad \text{p.w.:} \quad h_i(w) = 0, \; i = 1, \ldots, l
\end{equation}$$
- Lagrangian: 
$$\begin{equation}
\mathcal{L}(w, \beta) = f(w) + \sum_{i=1}^l \beta_i h_i(w)
\end{equation}$$ 
gdzie $\beta_i$ to mnoÅ¼niki Lagrangeâ€™a.

- RozwiÄ…zywanie problemu:
	Znalezienie ekstremum polega na speÅ‚nieniu warunkÃ³w:
$$\begin{equation}
\frac{\partial \mathcal{L}}{\partial w_i} = 0 \quad \text{i} \quad \frac{\partial \mathcal{L}}{\partial \beta_i} = 0
\end{equation}$$
### WiÄ™zy w postaci nierÃ³wnoÅ›ci

- MetodÄ™ moÅ¼na rozszerzyÄ‡ na problemy z wiÄ™zami nierÃ³wnoÅ›ciowymi:
$$\begin{equation}
\min_{w} f(w) \quad \text{p.w.:} \quad g_i(w) \leq 0, \; h_i(w) = 0
\end{equation}$$
- UogÃ³lniony lagrangian przyjmuje postaÄ‡:
$$\begin{equation}
\mathcal{L}(w, \alpha, \beta) = f(w) + \sum_{i=1}^k \alpha_i g_i(w) + \sum_{i=1}^l \beta_i h_i(w)
\end{equation}$$ 
gdzie $\alpha_i \geq 0$.


### Problem pierwotny i dualny

- **Definicja problemu pierwotnego**: $$ \theta _{p}(w) = \max _{\alpha ,\beta :\alpha _{i} \ge 0} \mathcal {L}(w,\alpha ,\beta )$$$$ p^{*} = \min _{w} \theta _{p}(w) = \min _{w} \max _{\alpha ,\beta : \alpha _{i }\ge 0 } \mathcal {L}(w,\alpha ,\beta )$$
- **Definicja problemu dualnego**: 
$$\begin{equation}
\theta_d(\alpha, \beta) = \min_{w} \mathcal{L}(w, \alpha, \beta)
\end{equation}$$ 
$$\begin{equation}
d^{*} = \max _{\alpha ,\beta : \alpha
_{i}\ge 0} \theta _{d}(\alpha, \beta) = \max _{\alpha ,\beta : \alpha _{i}\ge 0} \min _{w} \mathcal {L}(w,\alpha ,\beta )
\end{equation}$$
- **Relacja miÄ™dzy problemem pierwotnym i dualnym**: 
$$\begin{equation}
d^* \leq p^* 
\end{equation}$$

### Warunki Karusha-Kuhna-Tuckera (KKT)

RÃ³wnoÅ›Ä‡ $d^* = p^*$ zachodzi pod warunkami:

â€¢ $f(w)$ i $g_i(w)$ sÄ… wypukÅ‚e,
â€¢ $h_i(w)$ sÄ… afiniczne $(h_i(w) = a_i^T w + b_i)$,
â€¢ Istnieje w, dla ktÃ³rego $g_i(w) < 0$.


Â Pod tymi warunkami istniejÄ…Â $ğ‘¤âˆ—$, $ğ›¼âˆ—$, $ğ›½âˆ—$, dla ktÃ³rychÂ $p^{*}=d^{*}=\mathcal {L} (w^{*},\alpha ^{*},\beta ^{*})$. PonadtoÂ Â $ğ‘¤âˆ—$, $ğ›¼âˆ—$, $ğ›½âˆ—$ speÅ‚niajÄ…:

1. $\frac{\partial \mathcal{L}}{\partial w_i} = 0$,
2. $\frac{\partial \mathcal{L}}{\partial \beta_i} = 0$,
3. $\alpha_i^* g_i(w^*) = 0$,
4. $g_i(w) \leq 0, \; \alpha_i \geq 0$.

Prawdziwe jest teÅ¼ twierdzenie odwrotne. JeÅ›li jakieÅ› parametryÂ ğ‘¤âˆ—,ğ›¼âˆ—,ğ›½âˆ—Â speÅ‚niajÄ… warunki KKT to sÄ… teÅ¼ rozwiÄ…zaniem problemu pierwotnego i dualnego.

## Klasyfikator SVM a Lagrange

Problem optymalizacyjny klasyfikatora SVM wyraziliÅ›my nastÄ™pujÄ…co: 
$$\begin{equation}
\min_{w, b} \frac{1}{2} \, \|w\|^2
\quad \text{p.w.} \quad y^{(j)} \left( w^T x^{(j)} + b \right) \geq 1, \, j=1,\dots,m
\end{equation}$$
PrzeksztaÅ‚cajÄ…c do postaci pasujÄ…cej do formalizmu uogÃ³lnionej metody Lagrange'a: 
$$\begin{equation}
\min_{w, b} \frac{1}{2} \, \|w\|^2
\quad \text{p.w.} \quad g_{j}(w,b) = 1 - y^{(j)}(w^{T}x^{(j)}+b) \le 0, \quad j= 1, \dots ,m
\end{equation}$$
Lagrangian dla tego problemu wyglÄ…da tak:
$$\begin{equation}
\mathcal {L}(w,b,\alpha ) = \frac{1}{2}||w||^{2} + \sum _{j=1}^{m} \alpha _{j}g_j(w,b)=
\end{equation}$$
$$\begin{equation}
= \frac{1}{2}||w||^{2} + \sum _{j=1}^{m} \alpha _{j}\left[1 - y^{(j)}(w^{T}x^{(j)}+b) \right]=
\end{equation}$$
$$\begin{equation}
=\frac{1}{2}||w||^{2} - \sum _{j=1}^{m} \alpha _{j}\left[ y^{(j)}(w^{T}x^{(j)}+b) -1\right]
\end{equation}$$
### PrzejÅ›cie do postaci dualnej
$$\begin{equation}
\mathcal{L}(w, b, \alpha) = \frac{1}{2} \|w\|^2 - \sum_{j=1}^{m} \alpha_j \left[ y^{(j)} \left(w^T x^{(j)} + b\right) - 1 \right]
\end{equation}$$
**Krok 1: Minimalizacja wzglÄ™dem $w$ i $b$**
- Pochodna wzglÄ™dem $w$: 
$$\begin{equation}
\nabla_w \mathcal{L}(w, b, \alpha) = w - \sum_{j=1}^{m} \alpha_j y^{(j)} x^{(j)} = 0 
\end{equation}$$ StÄ…d, po przeksztaÅ‚ceniu: 
$$\begin{equation}
w^* = \sum_{j=1}^{m} \alpha_j y^{(j)} x^{(j)} 
\end{equation}$$
- Pochodna wzglÄ™dem $b$: 
$$\begin{equation}
\nabla_b \mathcal{L}(w, b, \alpha) = \sum_{j=1}^{m} \alpha_j y^{(j)} = 0 
\end{equation}$$
**Krok 2: Podstawienie $w^*$ do Lagrangiana**
- Norma $w^*$ jest rÃ³wna: 
$$\begin{equation}
\|w^*\|^2 = \left\| \sum_{j=1}^{m} \alpha_j y^{(j)} x^{(j)} \right\|^2 = \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle 
\end{equation}$$
- Po podstawieniu do funkcji Lagrangiana, uzyskujemy: 
$$\begin{equation}
\mathcal{L}(w^*, b, \alpha) = \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{j=1}^{m} \alpha_j \left[ y^{(j)} \left( \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x^{(j)} \rangle + b \right) - 1 \right] = 
\end{equation}$$
$$\begin{equation}
= \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{j=1}^{m} \alpha_j y^{(j)} \left( \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x^{(j)} \rangle + b \right) + \sum_{j=1}^{m} \alpha_j = 
\end{equation}$$ 
$$\begin{equation}
= \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{j=1}^{m} \alpha_j y^{(j)} \sum_{i=1}^{m} \alpha_i y^{(i)} \langle x^{(i)}, x^{(j)} \rangle - b\sum_{j=1}^{m} \alpha_j y^{(j)}  + \sum_{j=1}^{m} \alpha_j = 
\end{equation}$$ 
$$\begin{equation}
= \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - \sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - b\sum_{j=1}^{m} \alpha_j y^{(j)} + \sum_{j=1}^{m} \alpha_j =
\end{equation}$$ 
$$\begin{equation}
= -\frac{1}{2}\sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle - b\sum_{j=1}^{m} \alpha_j y^{(j)}  + \sum_{j=1}^{m} \alpha_j
\end{equation}$$
- Z warunku KTT wynika $\sum_{j=1}^{m} \alpha_j y^{(j)} = 0$, zatem: $$ \mathcal{L}(w^*, b, \alpha) = \sum_{j=1}^{m} \alpha_j -\frac{1}{2}\sum_{i,j=1}^{m} \alpha_i \alpha_j y^{(i)} y^{(j)} \langle x^{(i)}, x^{(j)} \rangle$$
**Krok 3: Dualny problem optymalizacyjny**
Funkcja $\mathcal{L}(w^*, b, \alpha)$ jest teraz funkcjÄ… tylko w zmiennych $\alpha_j$, a naszym celem jest jej zmaksymalizowanie, przy zachowaniu odpowiednich ograniczeÅ„. Ostateczny dualny problem optymalizacyjny zapisujemy jako: $$\max_{\alpha} \left( \sum_{j=1}^{m} \alpha_j - \frac{1}{2} \sum_{i,j=1}^{m} y^{(i)} y^{(j)} \alpha_i \alpha_j \langle x^{(i)}, x^{(j)} \rangle \right)$$
Pod warunkiem:
- $\alpha_j \geq 0, \quad j = 1, \dots, m$
- $\sum_{j=1}^{m} \alpha_j y^{(j)} = 0$

SpeÅ‚nione sÄ… warunki KKT, zatem rozwiÄ…zanie tego problemu dualnego jest teÅ¼ rozwiÄ…zaniem naszego problemu pierwotnego.


Po rozwiÄ…zaniu problemu dualnego SVM otrzymujemy optymalne wartoÅ›ci mnoÅ¼nikÃ³w Lagrangeâ€™a $\alpha^*_j$. WartoÅ›Ä‡ $\alpha^*_j > 0$ wskazuje, Å¼e odpowiadajÄ…cy mu punkt $(x^{(j)}, y^{(j)})$ jest wektorem noÅ›nym. **(wektory noÅ›ne - przykÅ‚ady poÅ‚oÅ¼one najbliÅ¼ej hiperpowierzchni decyzyjnej)**.

Zatem podstawiajÄ…c $\alpha^*_j$:
$$\begin{equation}
w^* = \sum_{j=1}^{m} \alpha^*_j y^{(j)} x^{(j)}
\end{equation}$$ 
$$\begin{equation}
b^* = -\frac{1}{2} \left( \max_{j : y^{(j)} = -1} w^* \cdot x^{(j)} + \min_{j : y^{(j)} = 1} w^* \cdot x^{(j)} \right)
\end{equation}$$

# Pytania do wykÅ‚adu
1. Algorytm SVM polega na ___ marginesÃ³w geometrycznych.
2. Czy margines geometryczny danego przykÅ‚adu moÅ¼na zmieniÄ‡ przez mnoÅ¼enie przez skalar?
3. Dodatnia wartoÅ›Ä‡ marginesu funkcjonalnego Å›wiadczy o ___ .
4. Margines geometryczny dla zbioru uczÄ…cego to: ___ Â z marginesÃ³w otrzymanych dla kaÅ¼dego z przykÅ‚adÃ³w.
5. Co gwarantuje warunek: $$ y^{(j)}(w^{T}x^{(j)} + b) \geq 1,  j = 1,...,m$$ w trzeciej wersji problemu optymalizacyjnego SVM? 
6. Problem pierwotny dla uogÃ³lnionego Lagrangianu polega na najpierwÂ  ___ wzglÄ™dem ___ , a nastÄ™pnie na ___ wzglÄ™dem ___ .
7. Czy parametry uogÃ³lnionego Lagrangianu speÅ‚niajÄ…ce warunki KKT sÄ… rozwiÄ…zaniem problemu pierwotnego?
8. Czym sÄ… wektory noÅ›ne?
9. Czy w technice SVM mapowanie wejÅ›cia do wiÄ™cej wymiarowej przestrzeni wykonujemy w sposÃ³b jawny?
10. Co robi funkcja jÄ…drowa? JakÄ… wartoÅ›Ä‡ zwraca dla wektorÃ³w, ktÃ³re sÄ… do siebie podobne? Czego ta funkcja jest miarÄ…?
11. Czy warunek: "jeÅ›li mamy jakieÅ› mapowanieÂ $\phi$Â i zwiÄ…zane z nim jÄ…droÂ $K$Â to macierz jÄ…dra jest symetryczna i dodatnio okreÅ›lona" jest warunkiem wystarczajÄ…cym abyÂ $K$Â byÅ‚a jÄ…drem?
12. Czy metodÄ™ optymalizacji osiowej moÅ¼na zastosowaÄ‡ wprost do problemu SVM?